---
title: 朴素贝叶斯
date: 2022-11-26
author: 瑜琦
top: false
hide: false
cover: false
mathjax: true
categories: Mechine Learning
tags: 
  - 朴素贝叶斯
  - 分类算法
  - 监督学习
  - 李航统计学习方法
---

## 朴素贝叶斯算法

### 阅读提示

本篇文章主要基于李航《统计学习方法》第4章中的内容做出讲解，结合该书或许会对你有更大的帮助。**如果你具有机器学习的基础，我建议你跳过第二部分的引例，**并重点阅读文章最后我在贝叶斯算法上的理解这部分。**如果你是初次学习，我推荐你在阅读完引例后在开始学习朴素贝叶斯算法，**这样或许能帮助你更好的理解该算法。总之，如果你在阅读时发现文章中存在某些问题，或者自己有不理解的地方，我随时欢迎你找我交流讨论。

### 一、主要思想

对于给定的数据集，**首先基于特征条件独立假设学习输入和输出的联合概率分布；然后基于该模型，对于给定的输入x，利用贝叶斯定理求出后验概率最大的输出**。这段话引自李航原书，初次阅读可能会觉得晦涩难懂而直接被劝退。但是这确实是朴素贝叶斯分类器的基本思想，我希望你在阅读本篇文章后对这句话有不一样的理解。

### 二、引例

现在我们要制作一个苹果的筛选装置，我们希望在不破坏苹果表皮的前提下筛选出优质果和普通果。相比而言直接测果汁的含糖量可能会得到准确结果，但是这么做会破坏果实而无法售卖。

假设在以上的前提下我们可以测得的数据有：**颜色，大小，花纹，是否向阳生长**。而我们希望通过这些测得的数据判断一个苹果是否属于优质果（比如含糖量量超过10%）。

下面，我们在果实中进行取样，假设摘了5颗苹果并测得以下数据

| 标号 | 颜色 | 大小 | 花纹 | 是否向阳生长 | 品质 |
| :--: | :--: | :--: | :--: | :----------: | :--: |
|  1   |  红  |  大  | 条纹 |      否      | 优等 |
|  2   |  红  |  小  |  无  |      是      | 优等 |
|  3   |  黄  |  大  |  无  |      否      | 普通 |
|  4   |  黄  |  小  | 条纹 |      是      | 优等 |
|  5   |  红  |  小  | 条纹 |      是      | 普通 |

#### 1. 简单分类器

如果在不考虑这些记录中的参数只考虑最终结果时，我们可以做一个简单的分类器，如下所示。
$$
f(x) = \frac{优等果数量}{果实总数} = \frac{3}{5} \tag{2.1}
$$
这个公式你一定不陌生，这就是在中学学习的概率，对于一堆苹果，我们要判断这堆苹果里有多少优质果和普通果，第一件事就是对这堆苹果随机抽样一部分，然后根据抽样的这部分判断整体。现在给这个抽样概率一个新的名字：**先验概率**。

下面我们把前面说的一堆苹果无线扩大，最终它会扩大成全世界上的所有苹果，包括成熟的未成熟的等等。那么我们就可以用之前做的分类器(2.1)对全世界的苹果进行分类了。简而言之我们在拿到一个苹果时就可以认为它有60%的概率是优等果。**或许你会说，这和分类器不一样，传统的分类器都是输入一些数据，直接输出分类结果，而这里怎么输出一个概率呢？**没错这是我认为贝叶斯分类器和传统分类器不一样的第一个地方（或许你也能在这里记录一下）。

但是，我们开始做的简单分类器太差劲了，因为不管给它什么苹果，它都会告诉你这个苹果有60%的可能是优等果。如果给它一个没成熟的苹果，它也会说这个苹果有很大可能是优等果(60%)。显然这非常违背我们的常识，所以我们需要对这个分类器进行改进，而改进的方法就是告诉这个分类器一些 **“常识”**。

#### 2. 改进分类器

就像上文所说，我们需要给这个简单分类加一些常识进去，这样它或许会在分类时不会做出那么蠢的判断。而要引入的常识就是先前测得的数据（颜色、大小等等）。而怎么引入这些常识就是我们下一步要思考的问题。

其实要做到这一点，也不是什么难事，概率论中我们就知道了有条件概率这个东西，那么我们只需要把这些条件（或者说常识）加给分类器就行了。
$$
f(x) = p(果实品质|果实的颜色、大小等) = p(y|X) \tag{2.2}
$$

$$
p(y|X) = \frac{p(X,y)}{p(X)}\tag{2.3}
$$



这里用**X**表示所有的条件，对应的我们求分类器的方法也应该做出改变，根据条件概率公式可将(2.2)转化成(2.3)。但是联合概率p(X,y)没办法求出，所以只能在对它做个变换了，还是使用条件概率公式。
$$
p(X,y) = p(y)p(X|y) \tag{2.4}
$$
经过不懈努力我们得到了一个新的分类器，这个分类器也叫**后验概率**：
$$
f(x) = p(y|X) = \frac{p(y)p(X|y)}{p(X)}\tag{2.5}
$$
这个分类器相比于之前就更加完美了，因为我们不仅考虑了抽样结果，还带入了我们的常识修正原来的分类器，但是这才是朴素贝叶斯算法的开始。

### 三、朴素贝叶斯

#### 1、基本方法

假设输入空间$ X\subseteq R^n $ 表示每个样本属性有n个(维)，输出空间记作$Y=\{c_1,c_2……\} $。对一个特定的样本我们把它输出的类记作$ y $,现有如下数据集：
$$
T=\{(x_1,y_1),(x_2,y_2)……,(x_N,y_N)\}
$$
我们希望通过数据集$T$学习一个分类器，参考上边挑苹果的例子，我们已经得到了一个合适的函数，它可以计算出一个苹果所对应类别的概率，我们只需要选择对应概率最大的那个类别就可以进行分类了，所以我们的分类器可以有如下定义：
$$
y=f(x)=\arg\max_{c_k}\frac{p(y=c_k)p(X|y=c_k)}{p(X)}\tag{3.1}
$$
这里对分母使用全概率公式进行拆分
$$
P(X) = \sum_kp(y=c_k)p(X|y=c_k)\tag{3.2}
$$
虽然公式得到了，但是在求解时就会发现条件概率$p(X|y)$是一个指数级复杂度，在此就不做证明，你只需要知道$p(X|y)$是无法直接得出的即可。为此引入了以下假设……

#### 2、因何而朴素

上文我们得知$p(X|y)$的求解复杂的是指数级，而造成指数级复杂度的原因在于我们给分类器引入了太多的**“常识”**，换句话说，我们的输入特征太多了($X\subseteq R^n$)。为此必须给分类器一个假设来降低计算复杂度，所以对输入的**特征空间**约定**条件独立**这也是朴素贝叶斯算法朴素的地方。

通过条件独立假设我们可以进一步化简$p(X|y)$
$$
p(X|y)=p(x^{(1)},x^{(2)}……x^{(n)}|y=c_k)=\prod_{j=1}^{n}p(x^{(j)}|y=c_k)\tag{3.3}
$$
因此原本的分类器进一步化简成
$$
y=f(x)=\arg\max_{c_k}\frac{p(y=c_k)\prod_{j=1}^{n}p(x^{(j)}|y=c_k)}{\sum_kp(y=c_k)\prod_{j=1}^{n}p(x^{(j)}|y=c_k)}\tag{3.4}
$$
可以看到分母中对$y$求和使得对任意的$c_k$分母都相同，所以对于$\arg\max$函数来说只需要考虑分子部分，所以最终分类器为（对应李航书中公式4.7）
$$
y=f(x)=\arg\max_{c_k}{p(y=c_k)\prod_{j=1}^{n}p(x^{(j)}|y=c_k)}\tag{3.5}
$$
至此朴素贝叶斯的算法部分就推到结束了，例题可以参考李航书中的例4.1或下方例题。

#### 3、例题

回到前文中挑选苹果的例子

| 标号 | 颜色 | 大小 | 花纹 | 是否向阳生长 | 品质 |
| :--: | :--: | :--: | :--: | :----------: | :--: |
|  1   |  红  |  大  | 条纹 |      否      | 优等 |
|  2   |  红  |  小  |  无  |      是      | 优等 |
|  3   |  黄  |  大  |  无  |      否      | 普通 |
|  4   |  黄  |  小  | 条纹 |      是      | 优等 |
|  5   |  红  |  小  | 条纹 |      是      | 普通 |

假设现在有一个新的苹果需要估计它的品质

|   标号   | 颜色 | 大小 | 花纹 | 是否向阳生长 |  品质  |
| :------: | ---- | ---- | ---- | :----------: | :----: |
| 测试样本 | 黄   | 大   | 条纹 |      是      | **？** |

根据朴素贝叶斯算法，首先计算如下概率

先验概率：$p(y_{优等})=\frac{3}{5}$, $p(y_{普通})=\frac{2}{5}$

条件概率(*likelihood*): 

​	颜色: $p(x_{颜色}=黄|y_{优等})=\frac{1}{3}$, $p(x_{颜色}=黄|y_{普通})=\frac{1}{2}$

​	大小: $p(x_{大小}=大|y_{优等})=\frac{1}{3}$, $p(x_{大小}=大|y_{普通})=\frac{1}{2}$

​	花纹: $p(x_{花纹}=条纹|y_{优等})=\frac{2}{3}$, $p(x_{花纹}=条纹|y_{普通})=\frac{1}{2}$ 

​	向阳: $p(x_{向阳}=是|y_{优等})=\frac{2}{3}$, $p(x_{向阳}=是|y_{普通})=\frac{1}{2}$

接下来就可以计算后验概率，这里要使用条件独立假设
$$
p(y=优等)p(x_{颜色}=黄色,x_{大小}=大,x_{花纹}=条纹,x_{向阳}=是|y=优等)=\frac{3}{5}*\frac{1}{3}*\frac{1}{3}*\frac{2}{3}*\frac{2}{3}=\frac{12}{405}
$$

$$
p(y=普通)p(x_{颜色}=黄色,x_{大小}=大,x_{花纹}=条纹,x_{向阳}=是|y=普通)=\frac{2}{5}*\frac{1}{2}*\frac{1}{2}*\frac{1}{2}*\frac{1}{2}=\frac{2}{80}
$$

由此我们可以将该苹果分类为普通品质

#### 4、段落小结

**至此我们已经学习了朴素贝叶斯的基本算法，如果你觉得这已经足够了，那么就可以停止读下去并开始进行实践了**。如果你还想更深入的了解一些朴素贝叶斯算法的思想，我强烈建议你继续读下去。

### 四、后验概率最大化

在上文中经过计算找到了使后验概率$p(y|X)$最大的类别$c_{k}$，对于有机器学习基础的人来说，机器学习的过程中都会定义一个损失函数来作为学习器性能的衡量指标，这么做确实更具有说服力。而朴素贝叶斯算法中却没有损失函数作为评判指标，**接下来我会给分类器定义一个损失函数并证明在没有损失函数的情况下，朴素贝叶斯分类器通过使后验概率最大一样可以做到让定义的损失最小。**

假设定义0-1损失作为损失函数
$$
L(Y,f(X))=\begin{cases}
1,&Y\neq{f(X)}\\
0,&Y={f(X)}
\end{cases}\tag{4.1}
$$
这里$X$为学习器的输入，$f(X)$为学习器的输出结果，$Y$是真实的类别，不难看出0-1损失函数就是简单的把学习器误判的样本进行记录，忽略那些判断正确的样本。

可以对损失函数取函数期望
$$
R_{exp}(f)=\sum_{k=1}^{K}{E_{p(c_k|X)}[L(c_k,f(X))]}\tag{4.2}
$$
注意到这里用$c_k$对$Y$做了替换，因为对于一个输入，应该取遍所有可能的输出并加和才是分类器在当前样本上的总损失，然后可以对这个损失求函数期望，接下来将期望损失进行极小化并展开。（如果感觉此处数学公式难以理解，可以在学习概率论后再阅读）
$$
\begin{aligned}
	\arg\min_{f(x)}{\sum_{k=1}^{K}{E_{p(c_k|X)}[L(c_k,f(X))]}}
&=\arg\min_{f(x)}{\sum_{k=1}^{K}{L(c_k,f(x))p(c_k|X)}}\\
&=\arg\min_{f(x)}{\sum_{k=1}^{K}p(f(x)\neq{c_k}|X)}\\
&=\arg\min_{f(x)}{1-p(f(x)=c_k|X)}\\
&=\arg\max_{f(x)}{p(f(x)=c_k|X)}
\end{aligned}\tag{4.3}
$$
对$(4.3)$的解释：首先根据概率期望展开，将损失函数带入，因为$f(x)=c_k$的损失为$0$所以带入损失函数后被忽略，因为每个样本只属于一个类（定义）所以根据概率公式得到第三行，只取负数部分得到最终的第四行。

可以发现经过对期望损失的展开，最终得到了一个极大化目标，而这个极大化目标就是后验概率$p(y|X)$只是此处$y$用$f(x)$替代，而式$\arg\max_{f(x)}p(f(x)=c_k|X)$可以理解为：当分类器的输出为$c_k$时整个分类器的期望损失达到最小，**而在上文的求解过程中，我们已经计算了对于** $c_k$ **的不同取值对应的概率** $p$**，我们选择使** $p$ **达到最大的** $c_k$ **的过程就是降低分类器整体损失的过程。**

所以根据期望风险最小化准则就证明了我们的后验概率最大化分类器的合理性
$$
f(x)=\arg\max_{c_k}p(c_k|X)
$$

### 五、总结

#### 1、算法缺陷

经过一系列的推导，我们证明了朴素贝叶斯算法的流程以及在机器学习中的合理性。但是推导过程中不难发现该方法存在的一些缺陷，下面做一个简单的归类

1. **条件独立假设这个论断太强了**，在真正的分类问题中输入的属性之间往往不是相互独立的，比如光照和果实颜色(光照越充足果实更红)。
2. 这个算法好像不能处理属性值为连续值的输入
3. 在一些似然概率为0的时候，计算后验会导致整体的后验概率都为0

针对第一点，这是朴素贝叶斯算法的基本假设，是没办法解决这个问题的。针对第二点，我们**可以假设连续值都满足正态分布**，从而直接使用正态分布来计算连续属性的似然概率。针对第三点，可以应用**拉普拉斯平滑**避免后验概率为0。具体可以参考周志华《机器学习》（西瓜书）的7.3节。

#### 2、回顾开篇

接下来我们重新思考文章开始的贝叶斯算法思想：**首先基于特征条件独立假设学习输入和输出的联合概率分布；然后基于该模型，对于给定的输入x，利用贝叶斯定理求出后验概率最大的输出。**我们计算$p(y)p(X|y)$的过程就是在求联合概率分布，而针对不同的$c_{k}$可以得到不同的后验概率，选择最优$c_{k}$的过程就是寻找后验概率最大的过程。

朴素贝叶斯算法也是我们在进行统计学习中的最基本思路，开始时我们根据在数据集的抽样，可以得到**先验概率**$p(y)$但是先验往往是不准确的，因为抽样结果永远无法代表整体。所以我们要在先验知识的基础上添加一些**“修正”**，而修正利用了样本中的某些属性，通过贝叶斯公式，我们可以把这些属性作为条件添加到先验概率中。这也是我对朴素贝叶斯算法的另一种解读。

